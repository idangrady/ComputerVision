{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, BatchNormalization, Conv2D, Dense, Flatten, Add, Dropout, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "def opticalFlowtoMat(set_, set_idx,up_points, show = False):\n",
    "    feature_params = dict( maxCorners = 100,\n",
    "                        qualityLevel = 0.3,\n",
    "                        minDistance = 7,\n",
    "                        blockSize = 7 )\n",
    "    # Parameters for lucas kanade optical flow\n",
    "    lk_params = dict( winSize  = (15, 15),\n",
    "                    maxLevel = 2,\n",
    "                    criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "    cap = cv.VideoCapture(f\"D:/github_/ComputerVision/Assignment_5//tv_human_interactions_videos/{set_[set_idx]}\") # here we should add the video dir r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4'\n",
    "    _, oldFrame = cap.read()\n",
    "\n",
    "    old_gray = cv.cvtColor(oldFrame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    # get the values to track and find their corners \n",
    "    oldPoints = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "    mask = np.zeros_like(old_gray)\n",
    "\n",
    "    color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame_new = cap.read()\n",
    "        if ret == True:\n",
    "            greyFrame_new = cv.cvtColor(frame_new, cv.COLOR_BGR2GRAY)\n",
    "            newPoints, status, error = cv.calcOpticalFlowPyrLK(old_gray, greyFrame_new, oldPoints, None, **lk_params) # where p0 are the points to track\n",
    "\n",
    "            if newPoints is not None:\n",
    "                good_new = newPoints[status==1]\n",
    "                good_old = oldPoints[status==1]\n",
    "            \n",
    "                for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                    a, b = new.ravel()\n",
    "                    c, d = old.ravel()\n",
    "                    mask = cv.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "                    frame = cv.circle(old_gray, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "\n",
    "\n",
    "            blank = np.ones((frame.shape[0], frame.shape[1]), dtype= mask.dtype) # This is  a black frame. \n",
    "            img = cv.add(blank, mask)\n",
    "\n",
    "            old_gray = greyFrame_new.copy()\n",
    "            oldPoints = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "    if(show):\n",
    "        cv.imshow('Frame',img)\n",
    "        cv.waitKey(0)\n",
    "    img = cv.resize(img, up_points, interpolation= cv.INTER_LINEAR)\n",
    "    img2 = cv2.merge((img,img,img))\n",
    "\n",
    "    return img2\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "up_width = 112\n",
    "up_height = 112\n",
    "up_points = (up_width, up_height)\n",
    "\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                    qualityLevel = 0.3,\n",
    "                    minDistance = 7,\n",
    "                    blockSize = 7 )\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15, 15),\n",
    "                maxLevel = 2,\n",
    "                criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap = cv.VideoCapture(r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4') # here we should add the video dir      cap = cv.VideoCapture(f\"tv_human_interactions_videos/{set_[set_idx]}\") # here we should add the video dir r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4'\n",
    "\n",
    "\n",
    "_, oldFrame = cap.read()\n",
    "# oldFrame = cv2.resize(oldFrame, up_points, interpolation= cv2.INTER_LINEAR)\n",
    "\n",
    "old_gray = cv.cvtColor(oldFrame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# get the values to track and find their corners \n",
    "oldPoints = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "mask = np.zeros_like(old_gray)\n",
    "\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame_new = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        # frame_new = cv2.resize(frame_new, up_points, interpolation= cv2.INTER_LINEAR)\n",
    "        greyFrame_new = cv.cvtColor(frame_new, cv.COLOR_BGR2GRAY)\n",
    "        newPoints, status, error = cv.calcOpticalFlowPyrLK(old_gray, greyFrame_new, oldPoints, None, **lk_params) # where p0 are the points to track\n",
    "\n",
    "        if newPoints is not None:\n",
    "            good_new = newPoints[status==1]\n",
    "            good_old = oldPoints[status==1]\n",
    "        \n",
    "            for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                a, b = new.ravel()\n",
    "                c, d = old.ravel()\n",
    "                mask = cv.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "                frame = cv.circle(old_gray, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "\n",
    "\n",
    "        blank = np.ones((frame.shape[0], frame.shape[1]), dtype= mask.dtype) # This is  a black frame. \n",
    "        img = cv.add(blank, mask)\n",
    "\n",
    "        old_gray = greyFrame_new.copy()\n",
    "        oldPoints = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "img = cv.resize(img, up_points, interpolation= cv.INTER_LINEAR)\n",
    "cv.imshow('Frame',img)\n",
    "cv.waitKey(0)\n",
    "\n",
    "\n",
    "#112 ,112 ,3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 112, 112, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 56, 56, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 56, 56, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 28, 28, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 26, 26, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18432)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               2359424   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,637,700\n",
      "Trainable params: 2,637,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "width =112\n",
    "height = 112\n",
    "\n",
    "def get_model(input_shape, outputShape,opt, lossFunc):\n",
    "    x = Input(shape=input_shape)\n",
    "    h = Conv2D(32, padding='same', kernel_size=(3,3), activation='relu')(x)\n",
    "    h = Conv2D(32, padding='same', kernel_size=(3,3), activation='relu')(x)\n",
    "    h = MaxPooling2D(pool_size=(2,2))(h)\n",
    "    h = Conv2D(64, padding='same', kernel_size=(3,3), activation='relu')(h)\n",
    "    h = Conv2D(64, padding='same', kernel_size=(3,3), activation='relu')(h)\n",
    "    h = MaxPooling2D(pool_size=(2,2))(h)\n",
    "    h = Conv2D(128, kernel_size=(3,3), activation='relu')(h)\n",
    "    h = Conv2D(128, kernel_size=(3,3), activation='relu')(h)\n",
    "    h = MaxPooling2D(pool_size=(2,2))(h)\n",
    "    h = Flatten()(h)\n",
    "    h = Dense(128, activation='relu')(h)\n",
    "    h = Dropout(.5)(h)\n",
    "    output = Dense(outputShape, activation='softmax')(h)\n",
    "\n",
    "    model = tf.keras.Model(inputs=x, outputs=output)\n",
    "    \n",
    "    model.compile(optimizer=opt, # 'adam'\n",
    "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # \n",
    "             metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "benchmark_model = get_model((width, height, 3), 4,'adam', 'categorical_crossentropy') # len(train_labels)\n",
    "benchmark_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 1\n",
    "Hand_shake  = {2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50}\n",
    "High_five = {1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48}\n",
    "Hug = {2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50}\t\n",
    "Kiss= {1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42}\n",
    "\n",
    "set_1 = [Hand_shake, High_five, Hug, Kiss]\n",
    "# Set 2\n",
    "Hand_shake_2 = {1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39}\n",
    "High_five_2={2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50}\n",
    "Hug_2= {1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48}\n",
    "Kiss_2 = {2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50}\n",
    "\n",
    "set_2 = [Hand_shake_2, High_five_2, Hug_2, Kiss_2]\n",
    "\n",
    "\n",
    "set_name = [\"Hand_shake\", \"High_five\", \"Hug\", \"Kiss\"]\n",
    "\n",
    "dic = {}\n",
    "labelList_set1= [None] * 51\n",
    "for idx, class_ in enumerate(set_1):\n",
    "    for num in class_:\n",
    "        if(labelList_set1[int(num)] is not None):\n",
    "            labelList_set1[int(num)].append(idx)\n",
    "        else:\n",
    "            labelList_set1[int(num)] = [idx]\n",
    "dic = {}\n",
    "labelList_set2= [None] * 51\n",
    "for idx, class_ in enumerate(set_2):\n",
    "    for num in class_:\n",
    "        if(labelList_set2[int(num)] is not None):\n",
    "            labelList_set2[int(num)].append(idx)\n",
    "        else:\n",
    "            labelList_set2[int(num)] = [idx]\n",
    "            \n",
    "z1 = np.zeros((51,4))\n",
    "z2 = np.zeros((51,4))\n",
    "idx =0\n",
    "for l1, l2 in zip(labelList_set1, labelList_set2) :\n",
    "    if(l1 is not None):\n",
    "        z1[idx,l1] = 1\n",
    "    if(l2 is not None):\n",
    "        z2[idx, l2] = 1\n",
    "    idx = idx+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1_1 = np.zeros((1,51))\n",
    "z1_2 = np.zeros((1,51))\n",
    "z1_3 = np.zeros((1,51))\n",
    "z1_4 = np.zeros((1,51))\n",
    "\n",
    "z1l = [z1_1, z1_2,z1_3, z1_4]\n",
    "\n",
    "\n",
    "z2_1 = np.zeros((1,51))\n",
    "z2_2 = np.zeros((1,51))\n",
    "z2_3 = np.zeros((1,51))\n",
    "z2_4 = np.zeros((1,51))\n",
    "\n",
    "z2l = [z2_1, z2_2,z2_3, z2_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for idx, row in enumerate(z1):\n",
    "    ss = [i for i, j in enumerate(row) if j == 1]\n",
    "    for i in ss:\n",
    "        z1l[i][:,idx] = 1\n",
    "\n",
    "\n",
    "for idx, row in enumerate(z2):\n",
    "    ss = [i for i, j in enumerate(row) if j == 1]\n",
    "    for i in ss:\n",
    "        z2l[i][:,idx] = 1\n",
    "\n",
    "        \n",
    "\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "video_no = 5  # change this to a number between [0, 100] and you can see a different training video from Set 2\n",
    "\n",
    "clip=VideoFileClip(f'TV-HI/tv_human_interactions_videos/{set_2[video_no]}')\n",
    "print(f'\\n\\nA video with the label - {set_2_label[video_no]}\\n')\n",
    "clip.ipython_display(width=380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set 1\n",
    "Hand_shake  = {2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50}\n",
    "High_five = {1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48}\n",
    "Hug = {2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50}\t\n",
    "Kiss= {1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42}\n",
    "\n",
    "set_1 = [Hand_shake, High_five, Hug, Kiss]\n",
    "# Set 2\n",
    "Hand_shake_2 = {1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39}\n",
    "High_five_2={2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50}\n",
    "Hug_2= {1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48}\n",
    "Kiss_2 = {2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50}\n",
    "\n",
    "set_2 = [Hand_shake_2, High_five_2, Hug_2, Kiss_2]\n",
    "\n",
    "\n",
    "set_name = [\"Hand_shake\", \"High_five\", \"Hug\", \"Kiss\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 2 to be used for train and validation \n",
      "\t100/['handShake_0001.avi', 'handShake_0003.avi', 'handShake_0004.avi', 'handShake_0005.avi', 'handShake_0006.avi', 'handShake_0007.avi', 'handShake_0008.avi', 'handShake_0009.avi', 'handShake_0010.avi', 'handShake_0011.avi', 'handShake_0012.avi', 'handShake_0013.avi', 'handShake_0017.avi', 'handShake_0022.avi', 'handShake_0023.avi', 'handShake_0029.avi', 'handShake_0030.avi', 'handShake_0031.avi', 'handShake_0033.avi', 'handShake_0034.avi', 'handShake_0035.avi', 'handShake_0036.avi', 'handShake_0037.avi', 'handShake_0038.avi', 'handShake_0039.avi', 'highFive_0002.avi', 'highFive_0003.avi', 'highFive_0004.avi', 'highFive_0005.avi', 'highFive_0014.avi', 'highFive_0015.avi', 'highFive_0016.avi', 'highFive_0017.avi', 'highFive_0018.avi', 'highFive_0019.avi', 'highFive_0020.avi', 'highFive_0021.avi', 'highFive_0022.avi', 'highFive_0026.avi', 'highFive_0036.avi', 'highFive_0037.avi', 'highFive_0038.avi', 'highFive_0039.avi', 'highFive_0040.avi', 'highFive_0041.avi', 'highFive_0042.avi', 'highFive_0043.avi', 'highFive_0046.avi', 'highFive_0049.avi', 'highFive_0050.avi', 'hug_0001.avi', 'hug_0005.avi', 'hug_0006.avi', 'hug_0007.avi', 'hug_0008.avi', 'hug_0009.avi', 'hug_0010.avi', 'hug_0013.avi', 'hug_0014.avi', 'hug_0019.avi', 'hug_0022.avi', 'hug_0023.avi', 'hug_0024.avi', 'hug_0025.avi', 'hug_0026.avi', 'hug_0028.avi', 'hug_0037.avi', 'hug_0038.avi', 'hug_0039.avi', 'hug_0040.avi', 'hug_0041.avi', 'hug_0043.avi', 'hug_0045.avi', 'hug_0047.avi', 'hug_0048.avi', 'kiss_0002.avi', 'kiss_0003.avi', 'kiss_0004.avi', 'kiss_0005.avi', 'kiss_0006.avi', 'kiss_0015.avi', 'kiss_0019.avi', 'kiss_0020.avi', 'kiss_0021.avi', 'kiss_0025.avi', 'kiss_0027.avi', 'kiss_0028.avi', 'kiss_0030.avi', 'kiss_0032.avi', 'kiss_0033.avi', 'kiss_0034.avi', 'kiss_0037.avi', 'kiss_0043.avi', 'kiss_0044.avi', 'kiss_0045.avi', 'kiss_0046.avi', 'kiss_0047.avi', 'kiss_0048.avi', 'kiss_0049.avi', 'kiss_0050.avi'] \n"
     ]
    }
   ],
   "source": [
    "#SET 1:\n",
    "set_1_ind =[[2,14,15,16,18,19,20,21,24,25,26,27,28,32,40,41,42,43,44,45,46,47,48,49,50],\n",
    "[ 1,6,7,8,9,10,11,12,13,23,24,25,27,28,29,30,31,32,33,34,35,44,45,47,48],\n",
    "[2,3,4,11,12,15,16,17,18,20,21,27,29,30,31,32,33,34,35,36,42,44,46,49,50],[ 1,7,8,9,10,11,12,13,14,16,17,18,22,23,24,26,29,31,35,36,38,39,40,41,42]]\n",
    "\n",
    "\n",
    "#SET 2:\n",
    "set_2_ind = [[1,3,4,5,6,7,8,9,10,11,12,13,17,22,23,29,30,31,33,34,35,36,37,38,39], [2,3,4,5,14,15,16,17,18,19,20,21,22,26,36,37,38,39,40,41,42,43,46,49,50], \n",
    "[1,5,6,7,8,9,10,13,14,19,22,23,24,25,26,28,37,38,39,40,41,43,45,47,48], [2,3,4,5,6,15,19,20,21,25,27,28,30,32,33,34,37,43,44,45,46,47,48,49,50]]\n",
    "\n",
    "set_name = [\"handShake\", \"highFive\", \"hug\", \"kiss\"]\n",
    "\n",
    "\n",
    "set_1 = [f'{set_name[c]}_{i:04d}.avi' for c in range(len(set_name)) for i in set_1_ind[c]]\n",
    "set_1_label =[f'{set_name[c]}' for c in range(len(set_name)) for i in set_1_ind[c]]\n",
    "\n",
    "set_2 = [f'{set_name[c]}_{i:04d}.avi' for c in range(len(set_name)) for i in set_2_ind[c]]\n",
    "set_2_label =[f'{set_name[c]}' for c in range(len(set_name)) for i in set_2_ind[c]]\n",
    "\n",
    "set_2 = [f'{set_name[c]}_{i:04d}.avi' for c in range(len(set_name)) for i in set_2_ind[c]]\n",
    "set_2_label =[f'{set_name[c]}' for c in range(len(set_name)) for i in set_2_ind[c]]\n",
    "\n",
    "# note that we ignore the negative / not found founds\n",
    "\n",
    "print(f'Set 2 to be used for train and validation \\n\\t{len(set_2)}/{set_2} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_processed_middle_frame(set_, set_idx):\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(f\"tv_human_interactions_videos/{set_[set_idx]}\")\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    middle_frame = int(length / 2)\n",
    "    cap.set(1, middle_frame)\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame / 255.0\n",
    "    frame = cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)\n",
    "    return frame\n",
    "\n",
    "def preprocess_labels(labels):\n",
    "    train_labels_names = set(labels)\n",
    "    #train_labels_nums = [i for i in range(len(train_labels_names))]\n",
    "    train_labels_y = []\n",
    "\n",
    "    train_labels_names = list(train_labels_names)\n",
    "\n",
    "    for class_name in labels:\n",
    "        i = train_labels_names.index(class_name)\n",
    "        train_labels_y.append(i)\n",
    "\n",
    "    return np.array(train_labels_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "width = 112\n",
    "height = 112\n",
    "dim = (width, height)\n",
    "\n",
    "'''Obtaining middle frames of test and train sets'''\n",
    "test_images_TV = [opticalFlowtoMat(set_1, set_idx, dim) for set_idx in range(len(set_1))]\n",
    "test_images_TV = np.array(test_images_TV)\n",
    "train_images_TV = [opticalFlowtoMat(set_2, set_idx,  dim) for set_idx in range(len(set_2))]\n",
    "train_images_TV = np.array(train_images_TV)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_y_TV = preprocess_labels(set_1_label)\n",
    "train_labels_y_TV = preprocess_labels(set_2_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 1s 24ms/step - loss: 0.2453 - accuracy: 0.9000\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.2463 - accuracy: 0.9300\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.2325 - accuracy: 0.9200\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.1299 - accuracy: 0.9700\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.1168 - accuracy: 0.9400\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0630 - accuracy: 0.9800\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0519 - accuracy: 0.9700\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0975 - accuracy: 0.9700\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.1259 - accuracy: 0.9500\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0418 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0524 - accuracy: 0.9800\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.0508 - accuracy: 0.9800\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0412 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 33ms/step - loss: 0.0368 - accuracy: 0.9900\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.0883 - accuracy: 0.9700\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0698 - accuracy: 0.9800\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0833 - accuracy: 0.9700\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0361 - accuracy: 0.9800\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0581 - accuracy: 0.9800\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0230 - accuracy: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28b63c05490>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_model.fit(train_images_TV,train_labels_y_TV , epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hand_shake_0002.avi'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cv.VideoCapture(f\"D:/github_/ComputerVision/Assignment_5/tv_human_interactions_videos/{set_1[2]}\") # here we should add the video dir r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(d.read()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal(olfimg, img):\n",
    "\n",
    "    feature_params = dict( maxCorners = 100,\n",
    "                    qualityLevel = 0.3,\n",
    "                    minDistance = 7,\n",
    "                    blockSize = 7 )\n",
    "    # Parameters for lucas kanade optical flow\n",
    "    lk_params = dict( winSize  = (15, 15),\n",
    "                    maxLevel = 2,\n",
    "                    criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "    cap = cv.VideoCapture(r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4') # here we should add the video dir      cap = cv.VideoCapture(f\"tv_human_interactions_videos/{set_[set_idx]}\") # here we should add the video dir r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4'\n",
    "    _, oldFrame = cap.read()\n",
    "\n",
    "    r_o,g_o,b_o = cv2.split(oldFrame)\n",
    "\n",
    "    mask_r = np.zeros_like(r_o)\n",
    "    mask_g = np.zeros_like(g_o)\n",
    "    mask_b = np.zeros_like(b_o)\n",
    "\n",
    "\n",
    "    oldPoints_r = cv.goodFeaturesToTrack(r_o, mask = None, **feature_params)\n",
    "    oldPoints_g = cv.goodFeaturesToTrack(g_o, mask = None, **feature_params)\n",
    "    oldPoints_b = cv.goodFeaturesToTrack(b_o, mask = None, **feature_params)\n",
    "\n",
    "    r_n,g_n,b_n = cv2.split(img)\n",
    "\n",
    "    newPoints_r, status_r, error_r = cv.calcOpticalFlowPyrLK(r_o, r_n, oldPoints_r, None, **lk_params) # where p0 are the points to track\n",
    "    newPoints_b, status_r, error_r = cv.calcOpticalFlowPyrLK(g_o, g_n, oldPoints_g, None, **lk_params) # where p0 are the points to track\n",
    "    newPoints_g, status_r, error_r = cv.calcOpticalFlowPyrLK(b_o, b_n, oldPoints_b, None, **lk_params) # where p0 are the points to track\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame_new = cap.read()\n",
    "\n",
    "        if ret == True:\n",
    "\n",
    "            # check for r\n",
    "            if newPoints_r is not None:\n",
    "                good_new_r = newPoints_r[status==1]\n",
    "                good_old_r = oldPoints_r[status==1]\n",
    "                for (old, new) in zip(good_old_r, good_new_r):\n",
    "                    a,b = new.ravel()\n",
    "                    c,d = old.ravel()\n",
    "                    mask_r = cv.line(mask,int(a), int(b), int(c), int(d),5,(255,0,0), 2)\n",
    "                        \n",
    "            # check for g\n",
    "\n",
    "            if newPoints_g is not None:\n",
    "                good_new_g = newPoints_g[status == 1]\n",
    "                good_old_g = oldPoints_g[status ==1]\n",
    "                for (old, new) in zip(good_old_g, good_new_g):\n",
    "                    a,b = new.ravel()\n",
    "                    c,d = old.ravel()\n",
    "                    mask_g = cv.line(mask,int(a), int(b), int(c), int(d),5,(0,255,0), 2)\n",
    "        \n",
    "            # check for b\n",
    "            if newPoints_b is not None:\n",
    "                good_new_b = newPoints_b[status==1]\n",
    "                good_old_b = oldPoints_b[status==1]\n",
    "\n",
    "                for (old, new) in zip(good_old_b, good_new_b):\n",
    "                    a,b = new.ravel()\n",
    "                    c,d = old.ravel()\n",
    "                    mask_b = cv.line(mask,int(a), int(b), int(c), int(d),5,(0,0,255), 2)\n",
    "\n",
    "\n",
    "            blank_r = np.ones((mask_r.shape[0], mask_r.shape[1]), dtype= mask.dtype) # This is  a black frame. \n",
    "            blank_g = np.ones((mask_g.shape[0], mask_g.shape[1]), dtype= mask.dtype) # This is  a black frame. \n",
    "            blank_b = np.ones((mask_b.shape[0], mask_b.shape[1]), dtype= mask.dtype) # This is  a black frame. \n",
    "\n",
    "            img_r = cv.add(blank_r, mask_r)\n",
    "            img_g = cv.add(blank_g, mask_g)\n",
    "            img_b = cv.add(blank_b, mask_b)\n",
    "\n",
    "            oldPoints_r= newPoints_r.copy()\n",
    "            oldPoints_g= newPoints_g.copy()\n",
    "            oldPoints_b= newPoints_b.copy()\n",
    "\n",
    "            # old_gray = greyFrame_new.copy()\n",
    "\n",
    "            oldPoints_r= newPoints_r.reshape(-1, 1, 2)\n",
    "            oldPoints_g= newPoints_g.reshape(-1, 1, 2)\n",
    "            oldPoints_b= newPoints_b.reshape(-1, 1, 2)\n",
    "            # oldPoints = good_new.reshape(-1, 1, 2)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        \n",
    "    img = cv.merge((img_r,img_g, img_b ))\n",
    "    return img\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                qualityLevel = 0.3,\n",
    "                minDistance = 7,\n",
    "                blockSize = 7 )\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15, 15),\n",
    "                maxLevel = 2,\n",
    "                criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap = cv.VideoCapture(r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4') # here we should add the video dir      cap = cv.VideoCapture(f\"tv_human_interactions_videos/{set_[set_idx]}\") # here we should add the video dir r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4'\n",
    "_, oldFrame = cap.read()\n",
    "print(oldFrame.shape)\n",
    "r_o,g_o,b_o = cv2.split(oldFrame)\n",
    "\n",
    "mask_r = np.zeros_like(r_o)\n",
    "mask_g = np.zeros_like(g_o)\n",
    "mask_b = np.zeros_like(b_o)\n",
    "\n",
    "\n",
    "oldPoints_r = cv.goodFeaturesToTrack(r_o, mask = None, **feature_params)\n",
    "oldPoints_g = cv.goodFeaturesToTrack(g_o, mask = None, **feature_params)\n",
    "oldPoints_b = cv.goodFeaturesToTrack(b_o, mask = None, **feature_params)\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame_new = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "\n",
    "        r_n,g_n,b_n = cv2.split(frame_new)\n",
    "\n",
    "        newPoints_r, status_r, error_r = cv.calcOpticalFlowPyrLK(r_o, r_n, oldPoints_r, None, **lk_params) # where p0 are the points to track\n",
    "        newPoints_b, status_r, error_r = cv.calcOpticalFlowPyrLK(g_o, g_n, oldPoints_g, None, **lk_params) # where p0 are the points to track\n",
    "        newPoints_g, status_r, error_r = cv.calcOpticalFlowPyrLK(b_o, b_n, oldPoints_b, None, **lk_params) # where p0 are the points to track\n",
    "\n",
    "        print(\"R\")\n",
    "\n",
    "        # check for r\n",
    "        if newPoints_r is not None:\n",
    "            print(newPoints_r)\n",
    "            good_new_r = newPoints_r[status==1]\n",
    "            good_old_r = oldPoints_r[status==1]\n",
    "            for (old, new) in zip(good_old_r, good_new_r):\n",
    "                a,b = new.ravel()\n",
    "                c,d = old.ravel()\n",
    "                mask_r = cv.line(mask,int(a), int(b), int(c), int(d),5,(255,0,0), 2)\n",
    "        print(\"G\")\n",
    "        # check for g\n",
    "\n",
    "        if newPoints_g is not None:\n",
    "            good_new_g = newPoints_g[status == 1]\n",
    "            good_old_g = oldPoints_g[status ==1]\n",
    "            for (old, new) in zip(good_old_g, good_new_g):\n",
    "                a,b = new.ravel()\n",
    "                c,d = old.ravel()\n",
    "                mask_g = cv.line(mask,int(a), int(b), int(c), int(d),5,(0,255,0), 2)\n",
    "        print(\"B\")\n",
    "\n",
    "        # check for b\n",
    "        if newPoints_b is not None:\n",
    "            good_new_b = newPoints_b[status==1]\n",
    "            good_old_b = oldPoints_b[status==1]\n",
    "\n",
    "            for (old, new) in zip(good_old_b, good_new_b):\n",
    "                a,b = new.ravel()\n",
    "                c,d = old.ravel()\n",
    "                mask_b = cv.line(mask,int(a), int(b), int(c), int(d),5,(0,0,255), 2)\n",
    "\n",
    "\n",
    "        print(\"Round 1\")\n",
    "\n",
    "        blank_r = np.ones((mask_r.shape[0], mask_r.shape[1]), dtype= mask.dtype) # This is  a black frame. \n",
    "        blank_g = np.ones((mask_g.shape[0], mask_g.shape[1]), dtype= mask.dtype) # This is  a black frame. \n",
    "        blank_b = np.ones((mask_b.shape[0], mask_b.shape[1]), dtype= mask.dtype) # This is  a black frame. \n",
    "\n",
    "        img_r = cv.add(blank_r, mask_r)\n",
    "        img_g = cv.add(blank_g, mask_g)\n",
    "        img_b = cv.add(blank_b, mask_b)\n",
    "\n",
    "        oldPoints_r= newPoints_r.copy()\n",
    "        oldPoints_g= newPoints_g.copy()\n",
    "        oldPoints_b= newPoints_b.copy()\n",
    "\n",
    "        # old_gray = greyFrame_new.copy()\n",
    "\n",
    "        oldPoints_r= newPoints_r.reshape(-1, 1, 2)\n",
    "        oldPoints_g= newPoints_g.reshape(-1, 1, 2)\n",
    "        oldPoints_b= newPoints_b.reshape(-1, 1, 2)\n",
    "        # oldPoints = good_new.reshape(-1, 1, 2)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "    \n",
    "img = cv.merge((img_r,img_g, img_b ))\n",
    "\n",
    "cv.imshow(img)\n",
    "cv.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[599.       301.00003 ]]\n",
      "\n",
      " [[312.84256  294.95316 ]]\n",
      "\n",
      " [[291.83835  255.8965  ]]\n",
      "\n",
      " [[516.       236.      ]]\n",
      "\n",
      " [[278.87076  228.98538 ]]\n",
      "\n",
      " [[438.00256  162.99986 ]]\n",
      "\n",
      " [[603.0002   309.00003 ]]\n",
      "\n",
      " [[295.838    269.9642  ]]\n",
      "\n",
      " [[328.0001    55.999844]]\n",
      "\n",
      " [[358.5343   302.9304  ]]\n",
      "\n",
      " [[334.00015   51.99962 ]]\n",
      "\n",
      " [[344.95374  113.9961  ]]\n",
      "\n",
      " [[321.00003   49.000065]]\n",
      "\n",
      " [[311.902    310.978   ]]\n",
      "\n",
      " [[476.81186  340.27673 ]]\n",
      "\n",
      " [[320.0003   162.99925 ]]\n",
      "\n",
      " [[517.99994  290.99976 ]]\n",
      "\n",
      " [[406.95242  101.905785]]\n",
      "\n",
      " [[400.55737  272.274   ]]\n",
      "\n",
      " [[355.61786  308.2187  ]]\n",
      "\n",
      " [[310.        31.      ]]]\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Canceled future for execute_request message before replies were done",
     "output_type": "error",
     "traceback": [
      "Error: Canceled future for execute_request message before replies were done",
      "at t.KernelShellFutureHandler.dispose (c:\\Users\\admin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1204175)",
      "at c:\\Users\\admin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223227",
      "at Map.forEach (<anonymous>)",
      "at v._clearKernelState (c:\\Users\\admin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1223212)",
      "at v.dispose (c:\\Users\\admin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:1216694)",
      "at c:\\Users\\admin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533674",
      "at t.swallowExceptions (c:\\Users\\admin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:913059)",
      "at dispose (c:\\Users\\admin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:533652)",
      "at t.RawSession.dispose (c:\\Users\\admin\\.vscode\\extensions\\ms-toolsai.jupyter-2022.3.1000901801\\out\\extension.js:2:537330)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "up_width = 112\n",
    "up_height = 112\n",
    "up_points = (up_width, up_height)\n",
    "\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                    qualityLevel = 0.3,\n",
    "                    minDistance = 7,\n",
    "                    blockSize = 7 )\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (15, 15),\n",
    "                maxLevel = 2,\n",
    "                criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap = cv.VideoCapture(r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4') # here we should add the video dir      cap = cv.VideoCapture(f\"tv_human_interactions_videos/{set_[set_idx]}\") # here we should add the video dir r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4'\n",
    "\n",
    "\n",
    "_, oldFrame = cap.read()\n",
    "# oldFrame = cv2.resize(oldFrame, up_points, interpolation= cv2.INTER_LINEAR)\n",
    "\n",
    "old_gray = cv.cvtColor(oldFrame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# get the values to track and find their corners \n",
    "oldPoints = cv.goodFeaturesToTrack(old_gray, mask = None, **feature_params)\n",
    "mask = np.zeros_like(old_gray)\n",
    "\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame_new = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        # frame_new = cv2.resize(frame_new, up_points, interpolation= cv2.INTER_LINEAR)\n",
    "        greyFrame_new = cv.cvtColor(frame_new, cv.COLOR_BGR2GRAY)\n",
    "        newPoints, status, error = cv.calcOpticalFlowPyrLK(old_gray, greyFrame_new, oldPoints, None, **lk_params) # where p0 are the points to track\n",
    "\n",
    "        if newPoints is not None:\n",
    "            print(newPoints)\n",
    "            break\n",
    "            \n",
    "            good_new = newPoints[status==1]\n",
    "            good_old = oldPoints[status==1]\n",
    "        \n",
    "            for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                a, b = new.ravel()\n",
    "                c, d = old.ravel()\n",
    "                mask = cv.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "                frame = cv.circle(old_gray, (int(a), int(b)), 5, color[i].tolist(), -1)\n",
    "\n",
    "\n",
    "        blank = np.ones((frame.shape[0], frame.shape[1]), dtype= mask.dtype) # This is  a black frame. \n",
    "        img = cv.add(blank, mask)\n",
    "\n",
    "        old_gray = greyFrame_new.copy()\n",
    "        oldPoints = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "img = cv.resize(img, up_points, interpolation= cv.INTER_LINEAR)\n",
    "cv.imshow('Frame',img)\n",
    "cv.waitKey(0)\n",
    "\n",
    "\n",
    "#112 ,112 ,3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_processed_middle_frame(set_, set_idx):\n",
    "    width = 112\n",
    "    height = 112\n",
    "    dim = (width, height)\n",
    "\n",
    "    cap = cv2.VideoCapture(f\"tv_human_interactions_videos/{set_[set_idx]}\")\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    middle_frame = int(length / 2)\n",
    "    cap.set(1, middle_frame)\n",
    "    ret, frame = cap.read()\n",
    "    frame = frame / 255.0\n",
    "    frame = cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op(frame1,frame2):\n",
    "    prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "    hsv = np.zeros_like(frame1)\n",
    "\n",
    "    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n",
    "    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "\n",
    "    return bgr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "def denseOpticalFlow(set_, set_idx, num_frames,dim ,show = False):\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(f\"tv_human_interactions_videos/{set_[set_idx]}\")\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    middle_frame = int(length / 2)\n",
    "    cap.set(1, middle_frame)\n",
    "    ret, frame1 = cap.read()\n",
    "    \n",
    "    prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "    hsv = np.zeros_like(frame1)\n",
    "    hsv[..., 1] = 255\n",
    "    i =0\n",
    "\n",
    "    out = np.zeros((dim[0],dim[1],3,num_frames))\n",
    "    while(i<num_frames):\n",
    "        ret, frame2 = cap.read()\n",
    "        if not ret:\n",
    "            print('No frames grabbed!')\n",
    "            break\n",
    "        next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "        flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        mag, ang = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "        hsv[..., 0] = ang*180/np.pi/2\n",
    "        hsv[..., 2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n",
    "        bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "        \n",
    "        if(num_frames>1):\n",
    "            bgr = cv2.resize(bgr, dim, interpolation=cv2.INTER_AREA)\n",
    "            out[:,:,:,i] = bgr\n",
    "        i+=1\n",
    "        if(show):\n",
    "            cv.imshow(\"Frame\", bgr)\n",
    "            cv.waitKey(50)\n",
    "    # print(f\"Out: {i}\")\n",
    "\n",
    "    bgr = cv2.resize(bgr, dim, interpolation=cv2.INTER_AREA)\n",
    "    if(i>1):\n",
    "        return out\n",
    "        \n",
    "    return bgr\n",
    "\n",
    "train_images_TV = [denseOpticalFlow(set_2, set_idx,1, (112,112), show = False) for set_idx in range(len(set_2))]\n",
    "train_images_TV = np.array(train_images_TV)\n",
    "\n",
    "test_images_TV = [denseOpticalFlow(set_1, set_idx,1, (112,112),show = False) for set_idx in range(len(set_1))]\n",
    "test_images_TV = np.array(test_images_TV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 112, 112, 3)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No frames grabbed!\n"
     ]
    }
   ],
   "source": [
    "cap = cv.VideoCapture(r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4') # here we should add the video dir      cap = cv.VideoCapture(f\"tv_human_interactions_videos/{set_[set_idx]}\") # here we should add the video dir r'D:/github_/ComputerVision/Assignment_5/__temp__.mp4'\n",
    "ret, frame1 = cap.read()\n",
    "prvs = cv.cvtColor(frame1, cv.COLOR_BGR2GRAY)\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "while(1):\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        print('No frames grabbed!')\n",
    "        break\n",
    "    next = cv.cvtColor(frame2, cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    mag, ang = cv.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang*180/np.pi/2\n",
    "    hsv[..., 2] = cv.normalize(mag, None, 0, 255, cv.NORM_MINMAX)\n",
    "    bgr = cv.cvtColor(hsv, cv.COLOR_HSV2BGR)\n",
    "    cv.imshow('frame2', bgr)\n",
    "    k = cv.waitKey(30) & 0xff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-c54ed5b33e70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbenchmark_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images_TV\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels_y_TV\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbenchmark_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbenchmark_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_images_TV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels_y_TV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "benchmark_model.fit(train_images_TV,train_labels_y_TV , epochs=20, batch_size=32)\n",
    "\n",
    "benchmark_model, test_acc = benchmark_model.evaluate(test_images_TV, test_labels_y_TV, verbose=2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96346c4e5d19eec5efc74fc6e2f47008814655f152417cc37e193ad39ae38f11"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
