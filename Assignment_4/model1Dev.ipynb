{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 784\n",
      "Number of classes: 10\n",
      "Number of self.Weights 5\n",
      "Hidden Layer 1: (3, 3, 1, 32)\n",
      "Hidden Layer 2: (3, 3, 32, 64)\n",
      "Hidden Layer 3: (3136, 120)\n",
      "Hidden Layer 4: (120, 10)\n",
      "Steps per epoch 500\n",
      "Epoch0.WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_1840\\2715518846.py:110: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  F = tf.compat.v1.layers.flatten(A)\n",
      "D:\\Anaconda_2\\envs\\tf2.4\\lib\\site-packages\\keras\\legacy_tf_layers\\core.py:541: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  return layer.apply(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Val acc: 0.647\n",
      "Epoch1...........Val acc: 0.7252\n",
      "Epoch2...........Val acc: 0.7521\n",
      "Epoch3...........Val acc: 0.7673\n",
      "Epoch4...........Val acc: 0.7761\n",
      "Epoch5...........Val acc: 0.7854\n",
      "Epoch6...........Val acc: 0.7922\n",
      "Epoch7...........Val acc: 0.7986\n",
      "Epoch8...........Val acc: 0.8055\n",
      "Epoch9...........Val acc: 0.8126\n",
      "Epoch10...........Val acc: 0.8183\n",
      "Epoch11...........Val acc: 0.825\n",
      "Epoch12...........Val acc: 0.8303\n",
      "Epoch13...........Val acc: 0.8338\n",
      "Epoch14...........Val acc: 0.837\n",
      "Epoch15...........Val acc: 0.8408\n",
      "Epoch16...........Val acc: 0.8429\n",
      "Epoch17...........Val acc: 0.8458\n",
      "Epoch18...........Val acc: 0.8482\n",
      "Epoch19...........Val acc: 0.8503\n",
      "Epoch20...........Val acc: 0.8516\n",
      "Epoch21...........Val acc: 0.8543\n",
      "Epoch22...........Val acc: 0.8565\n",
      "Epoch23...........Val acc: 0.8573\n",
      "Epoch24...........Val acc: 0.859\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def printIMG(x,y, idx):\n",
    "    label_dict = {\n",
    " 0: 'T-shirt/top',\n",
    " 1: 'Trouser',\n",
    " 2: 'Pullover',\n",
    " 3: 'Dress',\n",
    " 4: 'Coat',\n",
    " 5: 'Sandal',\n",
    " 6: 'Shirt',\n",
    " 7: 'Sneaker',\n",
    " 8: 'Bag',\n",
    " 9: 'Ankle boot',\n",
    "}\n",
    "\n",
    "    plt.imshow(x[idx])\n",
    "    plt.xlabel(label_dict[idx])\n",
    "# plt.imshow(x[0])\n",
    "\n",
    "def preprocessing():\n",
    "    (x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data() #tf.keras.datasets.mnist.load_data(path=\"mnist.npz\") # \n",
    "    x_train =x_train/255  \n",
    "    x_test = x_test/255 \n",
    "\n",
    "    # Make categorical for loss\n",
    "    y_train = tf.keras.utils.to_categorical(y_train)\n",
    "    y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "    # reshape for 3 dim\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "    x_test = x_test.reshape(-1,28,28,1)\n",
    "\n",
    "    \n",
    "    return (x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, num_input, layers ,lastDense, lr, dropout, debug=False):\n",
    "        self.hiddenlayers =layers\n",
    "        self.num_features = layers[0]\n",
    "        self.num_classes = layers[-1]\n",
    "        self.L =  len(layers) #len(self.Weights)+1  # len(layers)\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "        self.lr = lr\n",
    "        self.drouput = dropout\n",
    "\n",
    "        # paramerters\n",
    "        self.Weights = {}\n",
    "        self.biases = {}\n",
    "        self.dw = {}\n",
    "        self.db = {}\n",
    "\n",
    "        self.last_dense = lastDense\n",
    "        # should happen at the beginning when initlizing the \n",
    "        self.initilizeWeight()\n",
    "\n",
    "    def initilizeWeight(self):\n",
    "        # print(\"initilizeWeight\")\n",
    "        for i in range(1, self.L):\n",
    "            \n",
    "            self.Weights[i] =  self.hiddenlayers[i]#tf.Variable(tf.random.normal(shape =(self.hiddenlayers[i], self.hiddenlayers[i-1]))) #, initializer =  tf.contrib.layers.xavier_initializer(seed = 0)))\n",
    "            if self.debug: print(f\"self.Weights[i].shape: {self.Weights[i]} \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_loss(self, A, Y):\n",
    "        # print(\"compute_loss\")\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(Y,A)\n",
    "        # Mean over multiple dim\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    def forwardpass(self, X):\n",
    "        # print(\"forwardpass\")\n",
    "\n",
    "\n",
    "        if self.debug: print(f\"self.Weights: {len(self.Weights)}\")\n",
    "\n",
    "        \n",
    "        A = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "        for i in range(1, len(self.Weights)-2):\n",
    "\n",
    "            if self.debug: \n",
    "                print(f\" {i} {self.Weights[i].shape} \")  \n",
    "                input()\n",
    "            \n",
    "            Z = tf.nn.conv2d(A, self.Weights[i], strides = [1,1,1,1], padding = 'SAME')\n",
    "            \n",
    "\n",
    "            A = tf.nn.relu(Z)\n",
    "            \n",
    "\n",
    "            A = tf.nn.max_pool(A, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "            \n",
    "\n",
    "            if self.debug:\n",
    "                print(f\"A. tf.Max shape {i}: {A.shape}\") \n",
    "                print(f\"Z. tf.nn.conv2d shape {i}: {Z.shape}\")\n",
    "                input()\n",
    "\n",
    "        # print(f\"A shape before flat: {A.shape}\")\n",
    "        F = tf.compat.v1.layers.flatten(A)\n",
    "        # print(\"Before Z2\")\n",
    "        # input()\n",
    "        Z2 = tf.matmul(F,self.Weights[i+1]) \n",
    "        # print(\"After Z2\")\n",
    "        # input()\n",
    "        \n",
    "        #Z3 = tf.compat.v1.layers.dense(F, self.num_classes, activation=None)   #, activation_fn=None\n",
    "        #Z3= tf.keras.layers.Dense(F, self.num_classes, activation=None)\n",
    "        Z3 = Z2 = tf.matmul(Z2,self.Weights[i+2]) \n",
    "        # print(f\"F shape: {F.shape}\")\n",
    "        # print(f\"tf.transpose(self.last_dense): {tf.transpose(self.last_dense).shape}\")                \n",
    "        # print(f\"self.last_dense: {self.last_dense.shape}\")\n",
    "\n",
    "        # input()\n",
    "\n",
    "        # Z3 = tf.matmul(tf.transpose(F),self.last_dense) \n",
    "        # if self.debug: input()\n",
    "\n",
    "        A = Z3    \n",
    "        return A\n",
    "\n",
    "\n",
    "    def updateParams(self):\n",
    "        \"\"\" \n",
    "        We have all the weights and biases. now we need to update the weights with gd\n",
    "        The formula is \n",
    "        \"\"\"\n",
    "        if self.debug:print(\"updateParams\")\n",
    "        \n",
    "        # check also with assignning sub\n",
    "        for i in range(1, len(self.Weights)):\n",
    "            self.Weights[i].assign_sub(self.lr * self.dw[i])\n",
    "\n",
    "    def printInfoModel(self):\n",
    "        print(f\"Number of features: {self.num_features}\")\n",
    "        print(f\"Number of classes: {self.num_classes}\")\n",
    "        print(f\"Number of self.Weights {len(self.Weights)}\")\n",
    "\n",
    "        for i in range(1, len(self.hiddenlayers)-1):\n",
    "            print(f\"Hidden Layer {i}: {self.hiddenlayers[i].shape}\")\n",
    "\n",
    "    def computeLoss(self, Y, Z):\n",
    "        if self.debug:print(\"compute_loss\")\n",
    "        \n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(Y, Z) # try to use softmax\n",
    "\n",
    "        if self.debug: \n",
    "            print(f\"tf.reduce_mean(loss): {tf.reduce_mean(loss)}\")\n",
    "            input()\n",
    "\n",
    "        return (tf.reduce_mean(loss))\n",
    "\n",
    "    def train(self, x_train, y_train, x_test, y_test, epochs, steps_per_epoch, batch_size):\n",
    "\n",
    "        history = {\n",
    "            'val_loss':[],\n",
    "            'train_loss':[],\n",
    "            'val_acc':[]\n",
    "        }\n",
    "        \n",
    "        for e in range(0, epochs):\n",
    "            epoch_train_loss = 0.\n",
    "            print('Epoch{}'.format(e), end='.')\n",
    "            for i in range(0, steps_per_epoch):\n",
    "\n",
    "                x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
    "                y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
    "                if self.debug:\n",
    "                    print(len(x_batch))\n",
    "                    print(len(y_batch))\n",
    "                    print(f\"i*batch_size:(i+1)*batch_size: {[i*batch_size, (i+1)*batch_size]}\")\n",
    "                    input()\n",
    "                batch_loss = self.trainOnBatch(x_batch, y_batch)\n",
    "                epoch_train_loss += batch_loss\n",
    "                \n",
    "                if i%int(steps_per_epoch/10) == 0:\n",
    "                    print(end='.')\n",
    "                    \n",
    "            history['train_loss'].append(epoch_train_loss/steps_per_epoch)\n",
    "            val_A = self.forwardpass(x_test)\n",
    "\n",
    "            val_loss = self.compute_loss(val_A, y_test).numpy()\n",
    "            history['val_loss'].append(val_loss)\n",
    "            val_preds = self.predict(x_test)\n",
    "            val_acc =    np.mean(np.argmax(y_test, axis=1) == val_preds.numpy())\n",
    "            history['val_acc'].append(val_acc)\n",
    "            print('Val acc:',val_acc)\n",
    "        return history\n",
    "\n",
    "    def dropout(self, X):\n",
    "        a = tf.random.uniform((X.shape[0], X.shape[1]), dtype=tf.dtypes.float32)\n",
    "        b = tf.where(a<self.drouput , 0,1)\n",
    "        b = tf.cast(b, tf.dtypes.float32)\n",
    "        return (tf.math.multiply(b,X))\n",
    "\n",
    "\n",
    "    def trainOnBatch(self, X, Y):\n",
    "\n",
    "        X = tf.convert_to_tensor(X, dtype = tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y,dtype = tf.float32)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            for i in range(1, len(self.Weights)):\n",
    "                if self.debug: print(f\"tape: {i}\")\n",
    "\n",
    "                #X = self.dropout(X)\n",
    "                Z = self.forwardpass(X)\n",
    "                \n",
    "                loss = self.computeLoss(Y, Z)\n",
    "                self.dw[i] = tape.gradient(loss, self.Weights[i])\n",
    "                if self.debug:print(f\"self.dw[i]: {self.dw[i].shape}\")\n",
    "                \n",
    "\n",
    "                if self.debug:\n",
    "                    print(f\"type(loss): {type(loss)}\")\n",
    "                    print(f\"type(Weights[i]): {type(self.Weights[i])}\")\n",
    "                    print(f\"type(self.dw[i]): {type(self.dw[i])}\")\n",
    "\n",
    "                #self.db[i] = tape.gradient(loss, self.biases[i])\n",
    "        del tape\n",
    "        self.updateParams()\n",
    "        return loss.numpy()\n",
    "\n",
    "    def predict(self, X):\n",
    "        A = self.forwardpass(X)\n",
    "        return tf.argmax(tf.nn.softmax(A), axis=1)\n",
    "        \n",
    "\n",
    "\n",
    "n_classes = 10\n",
    "n_Input = 784\n",
    "weights = [\n",
    "    n_Input,\n",
    "    tf.compat.v1.get_variable('W1', shape=(3,3,1,32), initializer=tf.keras.initializers.glorot_normal()),\n",
    "     tf.compat.v1.get_variable('W2', shape=(3,3,32,64), initializer=tf.keras.initializers.glorot_normal()),\n",
    "    #  tf.compat.v1.get_variable('W2', shape=(3,3,64,128), initializer=tf.keras.initializers.glorot_normal()),\n",
    "    tf.compat.v1.get_variable('W3', shape=(7*7*64,120), initializer=tf.keras.initializers.glorot_normal()),\n",
    "   tf.compat.v1.get_variable('W4', shape=(120,n_classes), initializer=tf.keras.initializers.glorot_normal()),\n",
    "     n_classes\n",
    "]\n",
    "\n",
    "last = tf.compat.v1.get_variable('W2', shape=(120,n_classes), initializer=tf.keras.initializers.glorot_normal())\n",
    "\n",
    "\n",
    "net = CNN(n_Input,weights,last, 3e-3, 0.3, debug = False)\n",
    "net.printInfoModel()\n",
    "\n",
    "\n",
    "\n",
    "(x_train, y_train, x_test, y_test) = preprocessing()\n",
    "\n",
    "batch_size = 120\n",
    "epochs = 25\n",
    "steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
    "lr = 3e-3\n",
    "print('Steps per epoch', steps_per_epoch)\n",
    "\n",
    "\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "history = net.train(\n",
    "    x_train,y_train,\n",
    "    x_test, y_test,\n",
    "    epochs, steps_per_epoch,\n",
    "    batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b26b377b6521054fae3a74a2f14cb9ee384f6c6757db04c797c5f4188bc4f62d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf2.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
